{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfead8d3-4364-4435-a25d-59d4375f8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |█████████▌                      | 1.1 MB 49 kB/s eta 0:00:520\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/opt/anaconda3/lib/python3.9/http/client.py\", line 463, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/opt/anaconda3/lib/python3.9/http/client.py\", line 507, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.9/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.9/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 173, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 203, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 315, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 472, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 341, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 140, in __bool__\n",
      "    return any(self)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 128, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 32, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 204, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 295, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 305, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 550, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 239, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 145, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 144, in iter\n",
      "    for x in it:\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/opt/anaconda3/lib/python3.9/contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64184d73-5c07-482e-b5ee-ce9339bf3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45eeb6-f666-4491-8471-fc3b5ab2785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b4bf11-75e0-4e8a-956d-3a4c9a33e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 18:30:52,604 - distributed.diskutils - INFO - Found stale lock file and directory '/Users/mika.kimmins/projects/scalingpythonml/dask/dask-worker-space/worker-uq3h8rw9', purging\n",
      "2022-07-15 18:30:52,605 - distributed.diskutils - INFO - Found stale lock file and directory '/Users/mika.kimmins/projects/scalingpythonml/dask/dask-worker-space/worker-u47sujg_', purging\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "# when working with clusters, specify cluster config, n_workers and worker_size\n",
    "client = Client(n_workers=4, \n",
    "                       threads_per_worker=1,\n",
    "                       memory_limit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3186dc16-827a-4dbe-a924-1bedf59de550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fbcf7-e87d-4e99-998c-32ddbb8746ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/fairseq/blob/main/examples/translation/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ddfda-4fa9-4324-b2a6-a48c7802988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/abhishek/roberta-inference-5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b90750-2885-4d51-ac79-7b97eb2fe380",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenizers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tokenizers\n",
    "import string\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96221f4-b206-47cf-847f-b4f54f207d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 192\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "# ROBERTA_PATH = \"../input/roberta-base\"\n",
    "# TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "#     vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
    "#     merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
    "#     lowercase=True,\n",
    "#     add_prefix_space=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a176c-a011-4a08-a396-d8c96bfc9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/roberta-base\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e31ad0-5a96-4771-a14d-4a1ad257cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/arvissu/roberta-base-inference-v2-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546b6be-ac2f-4567-8133-14395a989eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c5bd8-9186-493f-b8eb-cfdd79c1f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model\n",
    "# download the datasets from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b017fcf-26b0-4bc5-b8d8-45148ffc20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchEssay(essay_id: str):\n",
    "    \"\"\"\n",
    "    Read the text file of the specific essay_id\n",
    "    \"\"\"\n",
    "    essay_path = os.path.join('../input/feedback-prize-effectiveness/train/', essay_id + '.txt')\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "# maybe?\n",
    "def fetchEssay_ddf(df, essay_id: str):\n",
    "    \"\"\"\n",
    "    Read the text file of the specific essay_id\n",
    "    \"\"\"\n",
    "    essay_path = os.path.join('../input/feedback-prize-effectiveness/train/', essay_id + '.txt')\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118bc75-6c57-466b-ba6e-7eb6d26447cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "FOLDS = 4\n",
    "lr = 2e-5\n",
    "SEED = 2018\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "accumulation_steps = 4\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db032274-2a5a-4f4e-95f1-8523aa011bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback:\n",
    "    def __init__(self):\n",
    "        self.loss = list()\n",
    "        self.model = list()\n",
    "    \n",
    "    def put(self, model, loss):\n",
    "        self.loss.append(loss)\n",
    "        self.model.append(model)\n",
    "\n",
    "    def get_model(self):\n",
    "        ind = np.argmin(self.loss)\n",
    "        return self.model[ind]\n",
    "\n",
    "    \n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = self.model(ids, mask)[0][:, 0, :]\n",
    "        pred = self.linear(x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class FeedBackDataset(Dataset):\n",
    "    def __init__(self, data, model_path, is_test=False):\n",
    "        self.data = data\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['discourse_text'].values[idx] + ' ' + self.tokenizer.sep_token*2  + ' '  + self.data['essay'].values[idx]\n",
    "        if not self.is_test:\n",
    "            target_value = self.data[y_cols].values[idx]\n",
    "      \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN\n",
    "        )['input_ids'] \n",
    "        \n",
    "                  \n",
    "        mask = [1]*len(inputs) + [0] * (MAX_LEN - len(inputs)) \n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        if len(inputs) != MAX_LEN:\n",
    "            inputs = inputs + [self.tokenizer.pad_token_id] * (MAX_LEN - len(inputs)) \n",
    "        ids = torch.tensor(inputs, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.is_test:\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            targets = torch.FloatTensor(target_value)\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "                'targets': targets\n",
    "            }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b767a1-31d9-4de0-88b0-ede639bd1579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ab23a-2ff4-408e-ab2c-da6b5171eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"Return the model and tokenizer\"\"\"ƒ\n",
    "    detoke = RobertaTokenizer.from_pretrained(model_path)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_path, return_dict=True)\n",
    "\n",
    "    return detoke, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d5af0-5a2e-4bf5-9049-46460181d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../input/roberta-base/'\n",
    "tokenizer, model_loaded = load_model(model_path)\n",
    "dmodel = dask.delayed(model_loaded.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be6346a4-61b0-4dd0-b322-a1f80fc68cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['discourse_effectiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1c67e-10f9-44d8-a017-6e7f2dc0f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask portion\n",
    "src = \"../input/feedback-prize-effectiveness/train.csv\"\n",
    "ddf = dd.read_csv(src)\n",
    "\n",
    "ddf['essay'] = ddf['essay_id'].map_partitions(\n",
    "    fetchessay\n",
    ")\n",
    "\n",
    "new_label = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n",
    "ddf['discourse_effectiveness']  = ddf['discourse_effectiveness'].map_partitions(\n",
    "   lambda x: new_label[x] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c91983-0ae3-44e8-9d46-4f752c2c5edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fb295-eea3-402e-b704-5af9a901d2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6894d2-c4e4-407c-81ae-3e705750a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/arvissu/roberta-base-training-notebook-1-epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9135e-6bb3-463d-9447-ffac6daf9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = list()\n",
    "kf = StratifiedKFold(n_splits=FOLDS)\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(df, y=df['essay_id'])):\n",
    "    print(f'fold {i+1}')\n",
    "    gc.collect()\n",
    "    \n",
    "    cb = callback()\n",
    "    train_loader = torch.utils.data.DataLoader(FeedBackDataset(df.loc[train_idx, :].reset_index(drop=True), model_path), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = torch.utils.data.DataLoader(FeedBackDataset(df.loc[valid_idx, :].reset_index(drop=True), model_path), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    \n",
    "    net = FeedBackModel(model_path)\n",
    "    net.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(net.parameters(), lr = lr)    \n",
    "    param_optimizer = list(net.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_train_optimization_steps = int(EPOCHS * len(train_loader) / accumulation_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(EPOCHS):  \n",
    "\n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        net.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        loss_list = []\n",
    "        val_loss_list = []\n",
    "        \n",
    "        for step, data in enumerate(tbar):\n",
    "\n",
    "            # get the inputs\n",
    "            input_ids = data['ids'].cuda()\n",
    "            input_masks = data['mask'].cuda()\n",
    "            targets = data['targets'].long().view(-1).cuda()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = net(input_ids,input_masks)\n",
    "                loss = loss_fn(pred, targets)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "\n",
    "            if step % accumulation_steps == 0 or step == len(tbar) - 1:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                \n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "            tbar.set_description(f\"Epoch {epoch + 1} Loss: {avg_loss} lr: {scheduler.get_last_lr()}\")\n",
    "        \n",
    "        net.eval()\n",
    "        avg_val_loss = 0.0   \n",
    "        tbar_val = tqdm(val_loader, file=sys.stdout)\n",
    "        for step, data in enumerate(tbar_val):\n",
    "\n",
    "            # get the inputs\n",
    "            input_ids = data['ids'].cuda()\n",
    "            input_masks = data['mask'].cuda()\n",
    "            targets = data['targets'].long().view(-1).cuda()\n",
    "            \n",
    "            pred = net(input_ids,input_masks)\n",
    "            loss = loss_fn(pred, targets)\n",
    "                \n",
    "            val_loss_list.append(loss.detach().cpu().item())\n",
    "            avg_val_loss = np.round(np.mean(val_loss_list), 4)\n",
    "\n",
    "            tbar_val.set_description(f\"Epoch {epoch + 1} Loss: {avg_val_loss}\")\n",
    "                    \n",
    "        cb.put(net, avg_val_loss)   \n",
    "        \n",
    "    model_list.append(cb.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451af95-11b8-43fe-be79-07e6991da4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done generating a model.\n",
    "model_target = \"roberta_modellist.pkl\"\n",
    "with open(model_target,\"wb\") as f:\n",
    "    pickle.dump(model_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1e96d-a759-4adb-825b-0fac657bc007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f534bb-47c8-4736-86de-1d59b1cae7d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      3\u001b[0m MAX_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m----> 6\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(FeedBackDataset(test_df, tokenizer, \u001b[38;5;28;01mTrue\u001b[39;00m), batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m model_list \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(model_target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(test_df), \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 512\n",
    "\n",
    "\n",
    "model_list = pickle.load(open(model_target, \"rb\"))\n",
    "test_pred = np.zeros((len(test_df), 3))\n",
    "\n",
    "# test_df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n",
    "test_ddf = dd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c268fc6-0d2d-42a3-b061-e5400912fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(test_df: pd.DataFrame, tokenizer, result):\n",
    "    test_loader = torch.utils.data.DataLoader(FeedBackDataset(test_df, tokenizer, True), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            input_ids = data['ids'].cuda()\n",
    "            input_masks = data['mask'].cuda()\n",
    "            pred = F.softmax(net(input_ids,input_masks))\n",
    "            result.extend(pred.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85307221-5ec8-4a70-8627-3873694ad800",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(NFOLDS):\n",
    "    print(f'start to inference fold : {idx}')\n",
    "    net = model_list[idx]\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    result = list()\n",
    "    predict(test_ddf, tokenizer, result)\n",
    "    test_pred += np.array(result)/NFOLDS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b2178-6cae-4725-9244-7a6e3f0036db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(NFOLDS):\n",
    "#     print(f'start to inference fold : {idx}')\n",
    "#     net = model_list[idx]\n",
    "#     net.eval()\n",
    "#     net.cuda()\n",
    "#     result = list()\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(test_loader):\n",
    "#             input_ids = data['ids'].cuda()\n",
    "#             input_masks = data['mask'].cuda()\n",
    "#             pred = F.softmax(net(input_ids,input_masks))\n",
    "#             result.extend(pred.cpu().detach().numpy())\n",
    "#     test_pred += np.array(result)/NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983743d-a318-4ba1-bf5d-dcab75b15806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our test set predictions\n",
    "test_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
